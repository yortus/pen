TODO 2020:
    [x] `float64`
    [ ] make `int32` into core syntax (no need to import)
    [ ] numeric literals
    [ ] integer ranges
    [ ] more numeric rules
    [ ] consider having global symbols/identifiers (instead of `import 'std'`)
        - `int32`, `true`, `false`, `null`, `char`, `string`, `float64`, `unicode`
    [ ] string literals have both text and ast forms, but null/boolean/numeric literals have only ast form. Inconsistency here?
    [ ] unicodeEscapeSequence
        - syntax in some languages:
            - JS/TS:    \uffff      \u{10ffff}
            - Ruby:     \uffff      \u{10ffff}      NB: there is another form with \u{...} too
            - C#:       \uffff      \U0010ffff
            - C:        \uffff      \U0010ffff
            - C++:      \uffff      \U0010ffff
            - Python:   \uffff      \U0010ffff
            - Rust:     \u{ffff}    \u{10ffff}      NB: 1-6 hex digits
            - OCaml:    \u{ffff}    \u{10ffff}      NB: 1-6 hex digits
            - Swift:    \u{ffff}    \u{10ffff}      NB: 1-8 hex digits
            - Go:       \uffff
            - Haskell:  \xffff      \x10ffff        NB: also supports decimal eg \99999 and octal eg \o777
            - CSS:      \ffff       \10ffff         NB: variable length
            - HTML:     &#xffff;                    NB: variable length; also supports decimal eg $#9999;
            - Java      \uffff      \uuuuuuffff     NB: always 4 hex digits
            - SQL:      -                           NB: no standard syntax?
        - some of the above languages also support named unicode escapes
        - conclusions:
            - need to consume/produce just the *digits* on the text side (ie not the \u or \x or &# or whatever parts)
            - need to allow for various number bases (usually hex, but also decimal, octal, maybe others)
        - could this be an extension of the core `character` function?
            - currently `character(min: string, max: string): Rule`
            - change to `character(arg: string | {min: string, max: string} | {base: number, minDigits: number, maxDigits: number}: Rule`
            - need core syntax for this, eg `unicode`, `u4`, `ux4`, `u1-6`, `u{1..6}`, `ux{1..8}`, `uo`, `ud`
    [ ] emit more efficient code for `sequence` (instrument first)
    [ ] implement FunctionExpressions (NB still fully static, a bit like generic type instantiations)
    [ ] link extension modules into emitted code
    [ ] document pre/post conds for `rule`s written in TS
        - (i) may modify state iff returns true; if returns false must leave state as it was before the call
        - meaning of `IMEM` for nodes is rule-specific

    [x] emit output for all node kinds:
    [x] complete `sys` (aka penlib)
    [x] link `sys.js` into emitted code
    [x] emit application as `sys.apply(fn, arg)` where `apply` does some runtime checks on `fn`
    [x] add a union type for `Datatype` (currently can be lambda|module|rule)
    [x] `reference` needs fixing: can't assert on target.kind for forward refs since it's not set yet
    [x] require main module to have a `start` rule (not necessarily exported)
    [x] add emit for exported `parse` and `unparse` functions, which call `start.parse` and `start.unparse`
    [x] update PEN syntax for string literals and char ranges as follows:
        [x] 'foo' ast only
        [x] "foo" both <->
        [x] `foo` text only
    [x] rename the three datatypes (so far) as follows (update comments in code too):
        [x] `Production` to `Rule`
        [x] `Function_` to `Lambda`
        [x] `Module` stays as-is
    [ ] unit tests for parse and unparse:
        [ ] anyChar
        [ ] character
        [ ] epsilon
        [ ] int32
        [ ] null
        [ ] boolean
        [ ] list
        [ ] maybe
        [ ] memoise
        [ ] not
        [ ] selection
        [ ] sequence
        [ ] string
        [ ] record
        [ ] zeroOrMore
    [ ] revise naming conventions - capitalise combinator/lambda fns? Rules?
    [ ] complete `std` (standard library)
        [x] memo (nb this is a lambda, not a rule)
        [x] i32
        [x] char (ie anychar)
        [x] intrinsic_true
        [x] intrinsic_false
        [x] intrinsic_null
        [x] zeroOrMore (nb this is a lambda, not a rule)
        [x] maybe / optional (nb this is a lambda, not a rule)
        [x] not (nb this is a lambda, not a rule)
        [x] epsilon, (ie Îµ-rule)
        [ ] textOnly / concrete
        [ ] astOnly / abstract
    [x] support importing things from `std` (but no other non-relative modspecs yet)
    [x] link `std.js` into emitted code
    [ ] possible to detect left recursion, and automatically insert `memoise` (then move memoize from `std` to `sys`)?
    [ ] better way of tracking `isFullyConsumed` for record/object nodes? 32 prop limit is limiting
    [ ] specify extension (and declaration) modules properly
        .d.pen for declaration file
        assumes a .js file is there with the impl
        can still type-check extensions using @check-js etc
        specify a file formal that allows simple concatenation of impl files
            is order important? Not if only containing function decls
            must we disallow import/require in impl files? Would it ever work? Maybe for non-relative module ids?
    [ ] Bikeshed names for parse/unparse rule:
        relation (sounds like some abstract math concept - could be confusing)
        rule
        pattern (too generic?)
        parser (one-sided - doesn't including unparsing)
        production (proper term for this. But one-sided? nah seems ok actually - same term used in both generative grammars and recognizers
        form
        combinator
        transducer
        axiom
        formula
        model
        shape
        schema
        definition
        

Next up:
    [x] Check if  Rep01 (source file graph) needs any changes for grammar update (don't think so) ANS: no changes required
    [x] Merge Module and ModuleExpression AST nodes? Why keep separate?
    [x] Update Rep02 (base AST) for grammar changes
        [x] Module and Record separation
        [x] Tuples gone
        [x] ThisExpression gone
        [x] ListExpression added
        [x] Binding and Pattern changes
        [x] note some node kinds have several variants with props that are either `true` or missing (eg `alias`)
        [x] anything else?
    [x] revise scope rules in light of module/record separation (record field names don't get added to a scope)
    [x] finalise phase 3
    [ ] resolve symbols for BindingLookupExpression
        [ ] expand `Symbol` to allow for namespace symbols, which link to their own module scope
        [ ] rename `namespace` prop to `moduleRef`?
    [ ] implement phase 4 starting from stuff in `[TODO]`
    [ ] remove `[TODO]` dir and contents


Grammar Rules affected by splitting modules and records:
    [x] Module
    [x] Binding (no dynamic, maybe change export syntax)
    [x] ExportBinding
    [x] StaticBinding (just a normal binding now, there is no dynamic)
    [x] DynamicBinding (remove / move to RecordField)
    [x] Pattern (see FooPattern changes below)
    [x] WildcardPattern (only for tuples - so remove for now)
    [x] RecordPattern (should be something like ModulePattern now)
    [x] FieldPattern (should be BindingPattern now?)
    [x] RecordExpression (whole lotta work)
    [x] ModuleExpression (inline module)



Other Grammar changes:
    [x] Introduce Lists and List Elements
    [x] module/import/export changes:
        [x] ModuleImportExpression
        [x] ModuleLiteralExpression
        [x] export as binding modifier, not pattern? pros/cons?




PEN Language features to consider in future:
    Records:
        [ ] shorthand fields
            - similar to shorthand properties in JS
            - need to also add optional commas between fields for disambiguation
        [ ] string literal field names
            - like `{'foo!': foo}` in JS
            - slightly shorter than current equivalent: `{['foo!'] = foo}`
    Tuples:
        [ ] re-introduce tuples later
        [ ] static concept only
        [ ] re-introduce `_` WildcardPattern for matching tuple elements
        [ ] tuples don't require parens
        [ ] add separate ParenthesisedExpression
        confirm use cases:
            [ ] eg can pass an ordered list of arguments to a function
            [ ] pattern matching - eg function can match passed-in tuple elements
            [ ] multivalue function returns???
            [ ] but what *are* fn return values in pen?
                [ ] cf generic type instantiation
    Patterns:
        [ ] generalise / add more
            - `ModulePattern` is currently simplified - support fully general/recursive?
            - re-introduce `TuplePattern` and `WildcardPattern`
            - See eg https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/pattern-matching













TODOs from the pegjs grammar file:
    RecordExpression:
        [x] shorthand when FieldName equals RHS Reference name, eg {Foo} short for {Foo = Foo}
        [x] need to append "," to these fields, otherwise abbiguous with sequences
        [x] make commas optional and valid for all fields (even non-shorthand ones) then? ANS: yes
        [x] remove shorthand altogether since it implies self-reference given scope & referencing rules
        [x] remove comma separators altogether in record literals since they are never need now to disambiguate
    Pattern:
        [ ] rest/spread element for RecordPattern and TuplePattern.
            - Q: why? show use case...
        [x] document special '_' identifier. ANS: has its own RESERVED token
        [x] commas between fields - required or optional or ...? ANS: optional, but significant in some cases
    TupleExpression:
        [x] are parentheses required? ANS: yes, otherwise ambiguous with comma-separated shorthand fields
          [ ] ahhh, but comma-separated shorthand fields no longer exist
          [ ] so now can make parens optional
        [ ] revisit parens requirement. When ambiguous exactly? Any alternative to make parens optional?
        [ ] Q: how do tuples map to AST nodes? Esp in 1-tuple case? Arrays always? Or?
    ImportExpression:
        [ ] current syntax is prefix operator notation, should use function application syntax instead? ie module('./foo')
    ApplicationExpression:
        [ ] support JSX-lke syntax alternative?
    Expression nodes in general:
        [x] revisit prop naming - eg in Function - pattern, expression are not descriptive, they are just the types (should be param(s), body?)
    Member Access:
        [ ] need syntax to access an element of a tuple expression



[ ] emit core imports if references - eg `Sequence`, `Selection`, etc
[ ] can we use dotted references? eg `myBlock.rule1` to `rule` exported from the block `myBlock`
    [ ] leaning toward yes for usefulness, eg imagine a mixed grammar with `json.array` and `yaml.array`
    [ ] if doing this, still need import aliases? eg `import foo as bar`. Can avoid collisions via dotted refs 
    [ ] related: can we `import * as m` from a module and then reference as `m.foo`?
    [ ] consider:
        - other uses of dot
          - alternative for 'match any char'?
        - whitespace handling, eg is `foo . bar` allowed? probably not
            - call `foo.bar` a 'nested binding' or 'composite binding' - treat it effectively as a single name/binding
        - readability






Older:
    [ ] rename `char` to `any-char`
    [x] rename `intrinsic-*` to null/boolean




Priorities:
    [x] make all parsers/unparsers into pure functions, put all outside any closures
    [x] never return `null` from parsers/unparsers, so can always destructure multiple returns.
    [x] new signature for parse:     `parse(src: string, pos: number, result: {ast: unknown, posá: number}): boolean`
    New signature for unparse:
        [x] step1: `unparse(ast: unknown, result: {src: string, astá: unknown}): boolean`
        [x] final: `unparse(ast: unknown, pos: number, result: {src: string, posá: number}): boolean`
        [x] fix `ZeroOrMore` to handle incremental consumption of more node types (ie not just strings)
        [x] still need `NO_NODE` in unparse template? ANS: no, removed
        [ ] still need `NO_NODE` in parse template?
    [x] should object & arrays just merge in sequences, like strings do? ANS: yes
        [x] implement this
        [x] remove spread/gather operators for objects and arrays
    [x] review `result` local in all parsers/unparsers - can we directly destructure some of these? N/A now
    [ ] combine parse/unparse templates into a single file
    [ ] break up templates into many files that cross-reference using TS namespace merging
    break up templates into separate files:
        [ ] penc needs to get them back into one file
    feature: modules:
        [ ] syntax eg `MyMod = import './my-mod.pen'` (looks like a normal binding except RHS is a relative path)
        [ ] break up existing functionality into modules
        [ ] pen modules `my-mod.pen`
        [ ] TS modules `special-thing.pen.ts`
            [ ] how to tell module format? don't rely on extension. Use hashbang? Comment at top? Just try to parse as pen, with fallback to ts
        [ ] built-in modules (may be either pen or ts format)
    type-check the AST to catch more errors and give better diagnostic feedback during compilation
    feature: `f64` intrinsic. Shortcut using `parseFloat` and `toString`, but with extra strictness checks
    revise `char` instrinsic - how to parse/unparse unicode escape sequences like `\u0032` in JSON strings?
    `parse`/`unparse` function signatures - with universal 'consumption progress' integer, incl on object keys (use bitfield)


Todo:
    [ ] consider common error: writing `Fn(X)` when meaning application `Fn<X>`, but getting sequence `Fn X` with no hint there's a user error
        [ ] type-checking the AST should help identify this error, since `Fn X` should be a type error since `Fn` is not a trancoder
    [ ] address known bug(s) in unit tests - see TODOs in unit test source files
    [ ] rename `'uniform'` everywhere to `"translit"` meaning a tranliterated string/char (cf abstract, concrete)


    review aggregate/element naming:
        [ ] `Record` vs `RecordField`/`RecordEntry`? (can be field or spread)
        [ ] `List` vs `ListElement`/`ListEntry`? (can be element or spread)
        [ ] `String` vs `Char`/`StringElement`?
    misc notation ideas:
        [ ] instead of `Maybe<>`, provide an 'epsilon' keyword ie `Maybe<A>` is same as `A | epsilon`
        [ ] use `&` or `;` as sequence operator, but make it always optional?
            [ ] this could allow `fn(arg)` syntax instead of `fn<arg>`, since can disambiguate with `nonFn ; (arg)`
        [ ] `not` operator
        [x] first-class intrinsic keywords for booleans (`true`|`false`)
        [x] first class intrinsic keyword for null (`null`? `void`? `()`? `nil`?)
        [ ] `OneOrMore` instead of `ZeroOrMore`? Then ZeroOrMore = maybe OneOrMore (call it `repeat` or `iter` or some such?)
    [ ] support `i32` literals in different bases (hex at least. See Java source code which does this)
    modules:
        [ ] list ideas...
    need a thoughoughly consistent approach to 'consuming' nodes:
        strings and objects both support incremental consumption now. Soon tuples will too.
        [x] so `NO_NODE`, `''` and `{}` can all mean 'no more input' in the right circumstances - generalise this
        [x] see/fix TODO comment in unparse code about commented-out assertion to check `result.N` is a 'residual' of `N`
    [ ] support debug mode a la multimethods, enable expensive `isResidualNode` checks only in debug mode
    [ ] in debug mode, wrap all parse/unparse functions in a wrapper function that checks pre/post/invariant conditions (eg `isResidualNode`)
    new PEN language features necessary to parse/unparse JSON:
        [x] computed field names, ie the `[NameRule]` part in `{[NameRule]: ValueRule}`
        [x] spread/gather fields for records, ie the `...RestRule` part in `{'foo': FooRule, ...RestRule}`
            NB: when parsing, it acts like the JS object spread operator in an object literal expression
            NB: when unparsing, it acts like the JS object rest operator in destructuring assignment (actually not - more like a gather operator)
            must it be like rest param - ie only one allowed and must come last?:
                for parsing you could have any number in any position without it being ambiguous
                for unparsing, it probably *does* need be be like a rest param (ie only one & comes last)
                actually have have multiple in any position - see JSON grammar - its about tracking what has been consumed so far in the current node
                so not really 'rest', more like 'gather'
        [x] lists (tuples), which map to JS arrays
            [x] which is the correct/best name? tuple? list? array? Use the term whose defn most closely matches. ANS: went with 'list'
        [x] rest/spread elements for lists/tuples
        [ ] character classes, including ranges and 'not' (or separate `Not<>` combinator)
        [ ] unicode character escapes (hex at least)
        [ ] `f64` floating point numbers
        [x] abstract literals `true`, `false`, and `null` (want these built-in or library-based?) ANS: library-based plz
        [x] `ZeroOrMore` combinator
        [ ] escape sequences in string literals (at least basic ones like `\n`)
    add mocha/chai unit tests to core pen module:
        [x] add expression grammar as a fixture
        [x] tests that parse various math expressions and expect a specific ast
        [x] tests that unparse various math expr asts and expect specific text
        [ ] parse/unparse round-trips on various math exprs that should/shouldn't succeed and/or get back original text
    BUG: rule names in grammars can shadow names referenced in parse/unparse templates:
        eg1, a rule named "String" in a unit test make i32 unparsing fail b/c String.fromCharCode referred to the local String decl
        eg2, ditto a rule named "Object" which messed up Object.keys calls
        [x] put 'user' rules into a nested scope, so they are never visible to built-in rules
        [ ] check whether 'user' rules can still interfere with each other through crafted names, eg `start`
    add UMD build using webpack, as per how `multimethods` does it:
        [ ] but first answer this: do we need umd for a command line tool?
    [x] move repo to yortus/penc
    `penc` should work as a command-line tool (and also an important node module?). What should the command output?:
        [ ] .d.ts file for parse/unparse functions, with AST types
        [ ] js code for parse/unparse functions
    [ ] address all the ts-ignore comments in the parse/unparse templates
    [ ] milestone: initial type analysis as part of compiling a grammar
    [ ] milestone: initial error diagnostic accumulation as part of parse/unparse internal operation
    [x] get unparse() working end-to-end; test it
    [ ] DRYify the very similar code in parse and unparse templates
    [x] add math expression grammar as test fixture
    [x] add API function to parse a grammar and return AST (temporary)
    [x] add API function to parse a grammar and spit out source for a `parse()` function:
    [x] add API function to parse a grammar and spit out source for an `unparse()` function
    [x] `i32`: get it working for natural numbers (ie no sign)
    [x] `i32`: support negative integers
    [x] `i32`: are exponents normally supported for int literals? What do other langs do? ANS: no exponents on int lits
    [ ] `i32`: if int is too large, we return `Fail`, but probably better to be some kind of error. What to do?
    [ ] `Module` and `Record` are basically the same - flesh out any differences and/or merge concepts
        [ ] difference: module can have non-exported bindings, whereas all record properties are visible outside
    [ ] `StringLiteral`: better word than 'uniform' for the "..." variant? (eg parallel, matching, constant, ordinary)
    [ ] `Sequence` comninator: revise code when we know if the 'type' of the seqence is 'string' or not
    [ ] review assert()s in template parse/unparse code - most should be static errors detectable from AST analysis
        [ ] sequences that produce multiple nodes
        [ ] record field values of NO_NODE
    [ ] theory: investigate memoisation for correctness
        [ ] when parsing, we only use `state.S` as the memo key. What about other state? Any example of incorrectness?
        [ ] do we need memoisation when unparsing? In which cases and why?
    [ ] way to add newline spacing between decls in emitted TS?
    [ ] only need `start` prop if binding value isn't a combinator factory call (eg string lits, simple delegation)
        [ ] `function isCombinator(astNode) {...}`
    [ ] doc resemblence of combinators to generic types that need 'instantiation' to make types
    [ ] support automatic insertion of Memo<...> around rhs of left-recursive rules (define subtasks...)




TODO: clarify terminology:
    'Combinator' - function that takes functions and returns a function:
    'Parser Combinator' - function that takes parse functions and returns a parse function:
        - so `Selection` and `Sequence` are parser combinators
        - what about `Record`? it's sort of hybrid
    'Parser Factory' - function that returns a parser:
        - so `Identifier` and `StringLiteral`

    'AST' vs 'text':
        - 'ast', consisting of nodes, one node
        - 'text', consisting of spans, one span


TODO: list perf tips (and benchmark them):
    avoid allocations
    avoid iterators - prefer `for (;;)`




Major research themes (focus on bi-directionality and issues arising):
    grammar bi-directionality - unification of parsing with unparsing (just transcoding in general - better term?)
    grammar extensibility and reuse
    grammar modularity
    strong typing
    sharing interfaces across grammars
    sharing interfaces with other tools
    information loss during parse/unparse - strategies (dimensions: lossy/lossless, in-band, out-of-band)
    stateful parsing/unparsing
    more encodings beside `text` - eg `binary`, `devnull` (time for multimethods?)


Secondary research themes (general things that need to be addressed somehow):
    parameterised rules
    error handling
    error correction
    incremental parsing/unparsing
    parallel parsing/unparsing
    performance
